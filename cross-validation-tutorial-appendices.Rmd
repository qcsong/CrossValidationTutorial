---
title: "Cross-Validation Tutorial Appendices"
author: "Q. Chelsea Song, Chen Tang, and Serena Wee"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

## Appendix A: Simulations to Further Demonstrate Observations 1, 2, and 3

This appendix provides the code and results for simulations that further demonstrate Observations 1, 2, and 3. Because each sample draw is influenced by sampling variation, results from the Shiny app may not always be as expected (i.e., by chance, it is possible for the calibrated model to fit better in the validation sample than in the calibration sample). To help illustrate the expected result for each of the three observations, we present three simulations, in each of which 1,000 samples were drawn from the population. For each sample, a model was fitted to the data, and then simulation results were obtained by averaging across the 1,000 model-fit results. Unless otherwise stated, we assume a quadratic relationship between arousal and performance. The overall simulation results support the observations that model generalizability decreases as (a) the model becomes more complex, (b) calibration sample size decreases, and (c) the effect size decreases. 

In each simulation, the sample was generated using the generate_sample function specified below. 



```{r}
# Define a function to generate samples for simulation 
generate_sample <- function(N, R2){ 
  mean_arousal <- 0 
  sd_arousal <- 3 
  arousal <- rnorm(n = N, mean = mean_arousal, sd = sd_arousal) 
  performance <- (300 - (arousal - 6) ^ 2) / 60 
   
  # add noise to performance score  
  snr <- R2 / (1 - R2)
  # calculate signal-to-noise ratio (based on Muirhead, 1985, p. 923) 
   
  sd_performance <- 0.6364
  # this sd is approximated using a separate simulation;  
  # simulation code is available from the authors 

  sd_noise <- sd_performance / sqrt(snr)
  # based on Hastie et al. (2009), p. 401 
  
  # generate and add noise
  noise <- rnorm(n = N, sd = sd_noise)  
  performance <- performance + noise
  
  dat <- data.frame(arousal = arousal, performance = performance) 
  return(dat) 
}
```


### Observation 1: the model overfits the calibration sample

The following lines of code allow the user to set (or change) the simulation parameters: population effect size, calibration sample size, and model complexity. 

```{r}
# Set a random seed 
set.seed(2020) 
 
# Set population effect size 

## Population R2 
R2_pop <- .25 

## Population MSE (corresponding to the population R2)  

MSE_pop <- 0.6364^2 / (R2_pop / (1 - R2_pop)) 
  
# Set calibration sample size 
n_cal <- 50 
  
# Set model complexity (degree of polynomial) 
degree <- 2
```

The following lines of code are used to randomly draw observations from the population, so as to generate 1,000 calibration samples. Then, in each calibration sample, a regression model is fitted, and the results from that model are stored. The overall simulation result is obtained by averaging across the 1,000 model-fit results.

```{r}
# Create vectors to store the calibration sample R-squared and MSE for each simulation trial 
R2 <- NA 
MSE <- NA 
 
for(i in 1:1000) { 
  # The following procedure is repeated 1000 times. 
  data <- generate_sample(N = n_cal, R2 = R2_pop) 
 
  # Fit the regression model to the sample and save  
  # the resulting model as “ob1_mod” 
  ob1_mod <- lm(performance ~ poly(arousal, degree), data = data) 
   
  # Store R-squared and MSE for each trial 
  R2[i] <- cor(ob1_mod$fitted.values, data$performance)^2 
  MSE[i] <- mean(summary(ob1_mod)$residuals^2) 
} 
 
# Calculate the average calibration R-squared and MSE, 
# averaged across 1000 simulation trials 
R2_cal <- mean(R2) 
MSE_cal <- mean(MSE)  

# Output the results 
cat(paste0( 
  "R-squared of the population model: ", round(R2_pop, 2), "\n", 
  "R-squared of the current model: ", round(R2_cal, 2), "\n", 
  "MSE of the population model: ", round(MSE_pop, 2), "\n", 
  "MSE of the current model: ", round(MSE_cal, 2), "\n"))
```

The output shows that the $R^2$ for the current model is larger than the population $R^2$. Meanwhile, $MSE$ for the current model is smaller than the population $MSE$. This is because the model captures sample-specific variation that is unrepresentative of the population. As a result, the model is overfitted to the calibration sample. 

### Observation 2: the model obtained from the calibration sample tends to not generalize well to new (validation) samples 

The code for this simulation applies the model obtained from the calibration sample to a new (validation) sample and checks the prediction accuracy.  

The following lines of code allow the user to set (or change) the simulation parameters: population effect size, calibration sample size, model complexity, and validation sample size. 

```{r}
# Set a random seed 
set.seed(2020) 
  
# Set population effect size 
R2_pop <- .25 
  
# Set calibration sample size 
n_cal <- 50 
  
# Set model complexity (degree of polynomial) 
degree <- 2 
  
# Set validation sample size 
n_val <- 1000 
```

The following lines of code are used to randomly draw observations from the population, so as to generate 1,000 calibration samples. Then, in each calibration sample, a regression model is fitted, and the results from that model are stored. The overall simulation result is obtained by averaging across the 1,000 model-fit results.

```{r}
# Create vectors to store the results for each trial 
R2_cal <- NA 
MSE_cal <- NA 
R2_val <- NA 
MSE_val <- NA 
 
for(i in 1:1000) { 
  # The following procedure is repeated 1000 times. 
  data_cal <- generate_sample(N = n_cal, R2 = R2_pop) 
  data_val <- generate_sample(N = n_val, R2 = R2_pop) 
  ob2_mod <- lm(performance ~ poly(arousal, degree), data = data_cal) 
  R2_cal[i] <- cor(ob2_mod$fitted.values, data_cal$performance)^2 
  MSE_cal[i] <- mean(summary(ob2_mod)$residuals^2) 
 
  # Use the fitted regression model to predict task performance  
  # from arousal in the validation sample 
  yhat_val <- predict(ob2_mod, data_val) 
 
  # Calculate validation R-squared and MSE 
  R2_val[i] <- cor(yhat_val, data_val$performance)^2 
  MSE_val[i] <- mean((yhat_val - data_val$performance)^2) 
} 
 
# Calculate the average validation sample R-squared and MSE, 
# averaged across 1000 trials 
R2_cal_mean <- mean(R2_cal) 
R2_val_mean <- mean(R2_val) 
MSE_cal_mean <- mean(MSE_cal) 
MSE_val_mean <- mean(MSE_val)  

# Output the results 
cat(paste0( 
  "Average Calibration R-squared: ", round(R2_cal_mean, 2), "\n", 
  "Average Validation R-squared: ", round(R2_val_mean, 2), "\n", 
  "Average Calibration MSE: ", round(MSE_cal_mean, 2), "\n", 
  "Average Validation MSE: ", round(MSE_val_mean, 2), "\n")) 
```

Notice that $R_{Val}^2$ is smaller than $R_{Cal}^2$: There is a 15% reduction in $R^2$($\frac{.27 − .23}{.27}=.15$). Also, $MSE_{Val}$ is larger than $MSE_{Cal}$: There is a 0.17 increase in MSE (1.31 –1.14 = 0.17). This suggests that when a model fitted on one (calibration) sample is used to make predictions in a new (validation) sample, the model performs less well in the new sample. 

### Observation 3: model generalizability is influenced by (a) model complexity, (b) sample size, and (c) effect size 

In the simulations for Observation 3, we vary (a) model complexity, (b) calibration sample size, and (c) effect size and compare the results with those obtained for Observation 2.  

#### Observation 3a: the model generalizes less well when the model is complex. 

This simulation explores how model complexity influences model generalizability. We increase the model complexity by specifying a cubic (instead of quadratic) regression model. Then, we apply the calibration models to new samples and obtain prediction accuracy. 

The following lines of code allow the user to set (or change) the simulation parameters: population effect size, calibration sample size, and model complexity.   

```{r}
## Set a random seed 
set.seed(2020) 
 
# Set population effect size 
R2_pop <- .25 
  
# Set calibration sample size 
n_cal <- 50 
  
# Set model complexity (degree of polynomial) 
degree <- 3 # Cubic regression, a more complex model 

n_val <- 1000 
```

The following lines of code are used to randomly draw observations from the population, so as to generate 1,000 calibration samples. Then, in each calibration sample, a regression model is fitted, and the results from that model are stored. The overall simulation result is obtained by averaging across the 1,000 model-fit results. 

```{r}
# Create vectors to store the calibration 
# and validation sample R-squared for each trial 
R2_cal <- NA 
MSE_cal <- NA 
R2_val <- NA 
MSE_val <- NA 
 
for(i in 1:1000) { 
  # The following procedures are repeated 1000 times 
  data_cal <- generate_sample(N = n_cal, R2 = R2_pop) 
  data_val <- generate_sample(N = n_val, R2 = R2_pop) 
  ob3a_mod <- lm(performance ~ poly(arousal, degree), data = data_cal) 
  R2_cal[i] <- cor(ob3a_mod$fitted.values, data_cal$performance)^2 
  MSE_cal[i] <- mean(summary(ob3a_mod)$residuals^2) 
  # Use the fitted regression model to predict task performance  
  # from arousal in the validation sample 
  yhat_val <- predict(ob3a_mod, data_val) 
   
  # Calculate validation R-squared and MSE 
  R2_val[i] <- cor(yhat_val, data_val$performance)^2 
  MSE_val[i] <- mean((yhat_val - data_val$performance)^2) 
} 
# Calculate the average validation sample R-squared, averaged across 1000 trials 
R2_cal_mean <- mean(R2_cal) 
R2_val_mean <- mean(R2_val) 
MSE_cal_mean <- mean(MSE_cal) 
MSE_val_mean <- mean(MSE_val) 
 
# Output the results 
cat(paste0( 
  "Average Calibration R-squared: ", round(R2_cal_mean, 2), "\n", 
  "Average Validation R-squared: ", round(R2_val_mean, 2), "\n", 
  "Average Calibration MSE: ", round(MSE_cal_mean, 2), "\n", 
  "Average Validation MSE: ", round(MSE_val_mean, 2), "\n"))
```

AAcross 1,000 trials, $R_{Val}^2$ is smaller for the cubic regression model ($R_{Val}^2$ = .20) than for the quadratic regression model ($R_{Val}^2$ = .23; see results for Observation 2). Similarly, $MSE_{Val}$ is larger for the cubic regression model ($MSE_{Val}$ = 1.43) than for the quadratic regression model ($MSE_{Val}$ = 1.31, see results for Observation 2). These results are consistent with Observation 3a: A complex model generalizes less well than a simpler model when applied to a new sample. 

#### Observation 3b: the model generalizes less well when calibration sample size is small. 

This simulation examines model generalizability for a smaller calibration sample size of 30 (as compared with 50). We apply the model obtained from the calibration sample to new (validation) samples and check the magnitude of prediction accuracy. 

The following lines of code allow the user to set (or change) the simulation parameters: population effect size, calibration sample size, model complexity, and validation sample size. 

```{r}
# Set a random seed 
set.seed(2020) 
 
# Set population effect size 
R2_pop <- .25 
  
# Set calibration sample size 
n_cal <- 30 # smaller sample size 
  
# Set model complexity (degree of polynomial) 
degree <- 2 
  
# Set validation sample size 
n_val <- 1000 
```

The following lines of code are used to randomly draw observations from the population, so as to generate 1,000 calibration samples. Then, in each calibration sample, a regression model is fitted, and the results from that model are stored. The overall simulation result is obtained by averaging across the 1,000 model-fit results.

```{r}
# Create vectors to store the calibration 
# and validation sample R-squared and MSE for each trial 
R2_cal <- NA 
MSE_cal <- NA 
R2_val <- NA 
MSE_val <- NA 
 
for(i in 1:1000) { 
  # The following procedures are repeated 1000 times 
  data_cal <- generate_sample(N = n_cal, R2 = R2_pop) 
  data_val <- generate_sample(N = n_val, R2 = R2_pop) 
  ob3b_mod <- lm(performance ~ poly(arousal, degree), data = data_cal) 
  R2_cal[i] <- cor(ob3b_mod$fitted.values, data_cal$performance)^2 
  MSE_cal[i] <- mean(summary(ob3b_mod)$residuals^2) 
 
  # Use the fitted regression model to predict task performance  
  # from arousal in the validation sample 
  yhat_val <- predict(ob3b_mod, data_val) 
   
  # Calculate validation R-squared and MSE 
  R2_val[i] <- cor(yhat_val, data_val$performance)^2 
  MSE_val[i] <- mean((yhat_val - data_val$performance)^2) 
} 
# Calculate the average calibration and validation sample R-squared and MSE, 
# averaged across 1000 trials 
R2_cal_mean <- mean(R2_cal) 
R2_val_mean <- mean(R2_val) 
MSE_cal_mean <- mean(MSE_cal) 
MSE_val_mean <- mean(MSE_val) 
 
# Output the results 
cat(paste0( 
  "Average Calibration R-squared: ", round(R2_cal_mean, 2), "\n", 
  "Average Validation R-squared: ", round(R2_val_mean, 2), "\n", 
  "Average Calibration MSE: ", round(MSE_cal_mean, 2), "\n", 
  "Average Validation MSE: ", round(MSE_val_mean, 2), "\n"))
```

Notice that across 1,000 trials, the average $RVal2$ is smaller when the same quadratic regression model was fitted on a small sample ($n$ = 30), average $RVal2$ = .22, than when it was fitted on a larger sample ($n$ = 50), average $RVal2$ = .23 (see the results for Observation 2). Similarly, the average $MSEVal$ is larger when the same quadratic regression model was fitted on a small sample ($n$ = 30), $MSEVal$ = 1.38, than when it was fitted on a larger sample ($n$ = 50), average $MSEVal$ = 1.31 (see the results for Observation 2). This suggests that a model fitted on a smaller calibration sample tends to generalize less well in a new sample, as compared with a model fitted on a larger calibration sample. 

#### Observation 3c: the model generalizes less well when the population effect size is small. 

This simulation examines model generalizability when the population effect size, $\rho^2$, is .04 (as compared with $\rho^2$ = .25). We apply the model obtained from the calibration sample to new samples and check the magnitude of prediction accuracy. 

The following lines of code set the parameters: population effect size, calibration sample size, model complexity, and validation sample size.  

```{r}
# Set a random seed 
set.seed(2020) 
 
# Set population effect size 
R2_pop <- .04 # small effect size 
  
# Set calibration sample size 
n_cal <- 50 
  
# Set model complexity (degree of polynomial) 
degree <- 2 
  
# Set validation sample size 
n_val <- 1000
```

The following lines of code run 1,000 simulation trials. In each trial, a calibration sample and a validation sample are drawn from the population; then, a regression model is fitted to the calibration sample and tested in the corresponding validation sample.

```{r}
# Create vectors to store the calibration 
# and validation sample R-squared for each trial 
R2_cal <- NA 
MSE_cal <- NA 
R2_val <- NA 
MSE_val <- NA 
 
for(i in 1:1000) { 
  # The following procedures are repeated 1000 times 
  data_cal <- generate_sample(N = n_cal, R2 = R2_pop) 
  data_val <- generate_sample(N = n_val, R2 = R2_pop) 
  ob3c_mod <- lm(performance ~ poly(arousal, degree), data = data_cal) 
  R2_cal[i] <- cor(ob3c_mod$fitted.values, data_cal$performance)^2 
  MSE_cal[i] <- mean(summary(ob3c_mod)$residuals^2) 
   
  # Use the fitted regression model to predict task performance  
  # from arousal in the validation sample 
  yhat_val <- predict(ob3c_mod, data_val) 
   
  # Calculate validation R-squared and MSE 
  R2_val[i] <- cor(yhat_val, data_val$performance)^2 
  MSE_val[i] <- mean((yhat_val - data_val$performance)^2) 
}
```

The following lines of code calculate the sample average of $R_2$ and $MSE$ of the calibration and validation samples by aggregating across 1,000 calibration samples and 1,000 validation samples, respectively. The average $R_{Cal}^2$ and $MSE_{Cal}$ are then compared with the average $R_{Val}^2$ and $MSE_{Val}$.

```{r}
# Calculate the average calibration and validation sample R-squared, averaged across 1000 trials 
R2_cal_mean <- mean(R2_cal) 
R2_val_mean <- mean(R2_val) 
MSE_cal_mean <- mean(MSE_cal) 
MSE_val_mean <- mean(MSE_val) 
 
# Output the results 
cat(paste0( 
  "Average Calibration R-squared: ", round(R2_cal_mean, 2), "\n", 
  "Average Validation R-squared: ", round(R2_val_mean, 2), "\n", 
  "Average Calibration MSE: ", round(MSE_cal_mean, 2), "\n", 
  "Average Validation MSE: ", round(MSE_val_mean, 2), "\n"))
```

Notice that when the population effect size is 0.04, across 1,000 trials, the average $R_{Val}^2$ is less than half the average $R_{Cal}^2$ (.03 vs. .08; 63% reduction vs. 15% reduction in the simulation for Observation 2), and the average $MSE_{Val}$ is larger than the average $MSE_{Cal}$ (10.48 vs. 9.10; 1.39 increase vs. 0.17 increase in the simulation for Observation 2). This suggests that when the population effect size is small, the model tends to generalize less well in a new sample.

## Appendix B: Demonstrations of k-Fold Cross-Validation and Monte Carlo Cross-Validation in R 

### k-fold cross-validation 

The following R code provides a step-by-step demonstration of a 5-fold cross-validation process: 

```{r}
# Set random seed 
set.seed(2020) 
 
# Sample size 
n <- 300 
 
# Set population effect size 
R2_pop <- .16 

# Generate a sample  
cv_data <- generate_sample(N = n, R2 = R2_pop) 
 
# Specify a cubic regression model 
degree <- 3 
 
# Fit the cubic regression model using the complete dataset and save the resulting model as “cv_original_mod” 
cv_original_mod <- lm(performance ~ poly(arousal, degree), data = cv_data) 
 
# Obtain R-squared and MSE of the model fitted to the complete dataset 
R2 <- cor(cv_original_mod$fitted.values, cv_data$performance)^2 
MSE <- mean(summary(cv_original_mod)$residuals^2) 
 
# Specify the number of folds 
k <- 5 
 
# Split the index of each observation into k equal subsets 
subsets <- split(x = 1:n, f = sort(rep_len(x = 1:k, length.out = n))) 
 
# Create vectors to store the cross-validated 

# R-squared and MSE for each fold 
R2_test <- NA 
MSE_test <- NA 
 
# Iterate through each fold 

for(i in 1:k) { 
  # Split the dataset into k equal subsets 
  ind_test <- subsets[[i]] 
   
  # For example, when i = 1, the first subset of data 
  # (1/5 of the original dataset) is specified as the test set 
  data_test <- cv_data[ind_test, ] 
   
  # Specify the rest of the data (4/5 of the original data) 
  # as the training set 
  data_train <- cv_data[-ind_test, ] 
   
  # Fit the cubic regression model to the training set 
  mod_tmp <- lm(performance ~ poly(arousal, degree), data = data_train) 
   
  # Test the model on the test set and calculate 
  # R-squared and MSE for this fold (repetition) 
  yhat_test <- predict(mod_tmp, data_test) 
  R2_test[i] <- cor(yhat_test, data_test$performance)^2 
  MSE_test[i] <- mean((yhat_test - data_test$performance)^2) 
} 
 
# Calculate the average cross-validated 

# R-squared and MSE (averaged across 5 folds) 
R2_kfold <- mean(R2_test) 
MSE_kfold <- mean(MSE_test) 
 
# Output the results 
cat(paste0( 
  "Model R-squared: ", round(R2, 2), "\n", 
  "Model MSE: ", round(MSE, 2), "\n", 
  "Cross-validated R-squared: ", round(R2_kfold, 2), "\n", 
  "Cross-validated MSE: ", round(MSE_kfold, 2), "\n"))
```

### Monte Carlo cross-validation 

The following R code provides a step-by-step demonstration of a Monte Carlo cross-validation process. In this example, we use 80% of the sample to train the model and the remaining 20% to test the model. This is repeated 100 times to obtain an estimate of the cross-validated $R^2$ and $MSE$. 

```{r}
## Using the same dataset and model as the above k-fold example 
 
set.seed(2020) 
 
# Specify number of repetitions 
rep <- 100 
 
# Set the test set size to 1/5 (20%) of the total sample size 
n_test <- ceiling(n / 5) # n_test = 60 for sample size 300 
 
# Create vectors to store the cross-validated 

# R-squared and MSE for each fold 

R2_test <- NA 
MSE_test <- NA 

# Iterate through each repetition 
for(i in 1:rep) { 
   # Repeat 100 times, each time randomly drawing 60 observations 
  # as the test set and the rest as the training set 
 
  ind_test <- sample(1:n, n_test) # randomly draw 60 numbers from 1 to 300 
   
  # use the 60 numbers as row indices and obtain test set 
  data_test <- cv_data[ind_test, ] 
   
  # Specify the remaining 240 observations as the training set 
  data_train <- cv_data[-ind_test, ] 
   
  # Fit the model with the training set  
  mod_temp <- lm(performance ~ poly(arousal, degree), data = data_train) 
  
  # Use the fitted model to make predictions in the test set 
  yhat_test <- predict(mod_temp, data_test) 
   
  # Save cross-validated R-squared and MSE for each repetition 
  R2_test[i] <- cor(yhat_test, data_test$performance)^2 
  MSE_test[i] <- mean((yhat_test - data_test$performance)^2)  
} 
 
# Calculate average cross-validated 

# R-squared (averaged across 100 repetitions) 
R2_mccv <- mean(R2_test) 
MSE_mccv <- mean(MSE_test) 
 
# Output the results 
cat(paste0( 
  "Model R-squared: ", round(R2, 2), "\n", 
  "Model MSE: ", round(MSE, 2), "\n", 
  "Cross-validated R-squared: ", round(R2_mccv, 2), "\n", 
  "Cross-validated MSE: ", round(MSE_mccv, 2), "\n"))
```

## Appendix C: Empirical Example Using the Machiavellianism Data Set 

In this example, we treat the 71,992 observations as the population and fit a regression model to obtain the population estimates. Regression results show that the population $R^2$ is .28, and the population $MSE$ is 0.45. The data set for this example was a cleaned version of the original data set and is available at the GitHub repository. 

```{r message = FALSE}
# If the caret package is not installed previously, 
# run the following line of code to install the package 
# install.packages("caret") 
 
# Load the ‘caret’ package 
library(caret) 
 
# Import data 
data <- read.csv("https://git.io/JfsiA") 
 
# Fit the model on the complete dataset 
mod <- lm(mach ~ age + as.factor(gender) + O + C + E + A + N, data = data) 
 
# Summary of the regression model fitted on the complete dataset 
R2_pop <- round(summary(mod)$r.squared, 2) 
MSE_pop <- round(mean(mod$residuals^2), 2) 
cat(paste0("R2 of the population model is ", R2_pop, ";\n", 
           "MSE of the population model is ", MSE_pop, "."))
```

### Model overfitting 

The following lines of code fit the same regression model with a calibration sample from the population. 

```{r}
# Load a calibration sample that was randomly drawn from the population 
sample_cal <- read.csv("https://git.io/JfsPt") 

# Note: This sample was randomly drawn from the complete Mach dataset using the code below 
# Interested readers can vary the random seed to test with different samples 
 
# set.seed(2020) 
# n <- 300 
# sample_cal <- data[sample(nrow(data), n), ] 
# Fit the regression model 

mod_cal <- lm(mach ~ age + as.factor(gender) + O + C + E + A + N, 
              data = sample_cal) 
 
# Calculate and save model fit results 
R2_cal <- round(summary(mod_cal)$r.squared, 2) 
MSE_cal <- round(mean(mod_cal$residuals^2), 2) 
 
# Print results 
cat(paste0("R2 of the calibration model is ", R2_cal, ";\n", 
           "MSE of the calibration model is ", MSE_cal, "."))
```

Note that the regression model for the calibration sample ($N$ = 300) has a larger $R^2$ and a smaller $MSE$ than the population model. 

### Cross-Validation

#### *k*-fold cross-validation

We carry out a 10-fold cross-validation with the following code. First, the `trainControl()` function is called, and the cross-validation method is specified as k-fold cross-validation (i.e., by specifying `method = "cv"`), with the number of folds equal to 10. The specified information is saved in an object named `kfold_train_control`. Then, the `train()` function is called to implement the k-fold cross-validation. The cross-validation results are saved in the object `“kfold_cv”`. 

```{r}
set.seed(2020) # Set a random seed to replicate results 
 
k <- 10 # Number of folds 
 
kfold_train_control <- trainControl(method = "cv", number = k) 
 
kfold_cv <- train(mach ~ age + as.factor(gender) + O + C + E + A + N,
                  data = sample_cal,method = "lm",
                  trControl = kfold_train_control)
 
# Calculate and save model fit results 
MSE_kfold <- round(mean(kfold_cv$resample$RMSE^2), 2) 
R2_kfold <- round(kfold_cv$results$Rsquared, 2) 
 
# Print results 
cat(paste0("k-fold cross-validated R2 is ", R2_kfold, ";\n", 
           "k-fold cross-validated MSE is ", MSE_kfold, ".")) 
```

It looks like there is a lot going on in this code, so let us explain each of the input arguments: First, `data = sample_cal` specifies that the `sample_cal` data set, the data set that was used in the original analysis, is used to conduct the cross-validation. Next, `method = “lm”` specifies the statistical model, as we need to indicate the model that will be used. In this case, we are using a linear (regression) model, and `method = “lm”` indicates that a linear regression model should be fitted. Finally, `trControl = kfold_train_control` tells the function that the 10-fold cross-validation method that we specified earlier should be used. 

All the results, including the cross-validated $R^2$ and $RMSE$ (square root of MSE), are saved in the object called `kfold_cv`.

#### Monte Carlo cross-validation (MCCV)

To conduct MCCV, we specify `method = "LGOCV"` (i.e., leave-group-out cross-validation, which is another term for Monte-Carlo cross-validation) in the `trainControl()` function. We set the number of repetitions to be 200 (`number = 200`; i.e., we ask for the train-then-test procedure to be conducted 200 times) and specify the proportion of data that should be randomly held out as a test set in each of the 200 repetitions. For example, `p = .8` means that 80% of the data set will be used as the training set, and therefore 20% (i.e., `1 - p`) of the data set will be used as the test set. These prespecifications are saved in an object, `mc_train_control`. Next, the `train()` function is used to implement MCCV. Finally, results are saved in an object, `mc_cv`. 

```{r}
set.seed(2020) # Set a random seed to replicate results
R <- 200 # Number of repetitions
mc_train_control <- trainControl(method="LGOCV", p=.8, number=R)
mc_cv <- train(mach ~ age + as.factor(gender) + O + C + E + A + N,
               data=sample_cal, method="lm", trControl=mc_train_control)
# Calculate and save results
MSE_mc <- round(mean(mc_cv$resample$RMSE^2), 2)
R2_mc <-  round(mc_cv$results$Rsquared, 2)
# Print results
cat(paste0("Monte Carlo cross-validated R2 is ", R2_mc, ";\n",
           "Monte Carlo cross-validated MSE is ", MSE_mc, "."))
```
